---
title: "RSelfStudy"
author: "Haoze Wu"
date: "2024-11-15"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(lintr.linters = lintr::linters_with_defaults(line_length_linter = NULL))
```

## Part I - Computing Quantitive Measures
Load the data from hiVotes.csv, scoreVotes.csv, scoreMetadata.csv
```{r}
hi_votes <- read.csv("hiVotes.csv")
score_votes <- read.csv("scoreVotes.csv")
score_metadata <- read.csv("scoreMetadata.csv")
company_metadata <- read.csv("companyMetadata.csv")
```
View the first 10 rows of each data frame.
```{r}
head(hi_votes, 10)
head(score_votes, 10)
head(score_metadata, 10)
```

Calculate the mean and standard deviation of `hiVote`, while rounding to 2 decimal places.
```{r}
mean_hi_vote <- round(mean(hi_votes$hiVote), 2)
std_hi_vote <- round(sd(hi_votes$hiVote), 2)
cat("Mean of hiVote: ", mean_hi_vote, "\n")
cat("Standard deviation of hiVote: ", std_hi_vote, "\n")
```
Calculate the mean and standard deviation of `scoreVote`, while rounding to 2 decimal places.
```{r}
mean_score_vote <- round(mean(score_votes$scoreVote), 2)
std_score_vote <- round(sd(score_votes$scoreVote), 2)
cat("Mean of scoreVote: ", mean_score_vote, "\n")
cat("Standard deviation of scoreVote: ", std_score_vote, "\n")
```
Calculate the mean and standard deviation of `scoreVote` for questions about “Wellbeing”, while rounding to 2 decimal places.
```{r}
wellbeing_metadata <- score_metadata[score_metadata$name == "Wellbeing", ]
wellbing_score_votes <- score_votes[
  score_votes$questionId %in% wellbeing_metadata$questionId,
]
mean_wellbeing_score_votes <- round(mean(wellbing_score_votes$scoreVote), 2)
std_wellbeing_score_votes <- round(sd(wellbing_score_votes$scoreVote), 2)
cat("Mean of scoreVote for questions about Wellbeing: ",
    mean_wellbeing_score_votes, "\n")
cat("Standard deviation of scoreVote for questions about Wellbeing: ",
    std_wellbeing_score_votes, "\n")
```
Calculate the mean and standard deviation of `scoreVote` for the question "On a scale from 1 to 10, how would you rate the work-related stress?"
```{r}
target_question <- "On a scale from 1 to 10, how would you rate the work-related stress?"
question_metadata <- score_metadata[
  score_metadata$question == target_question,
]
question_score_votes <- score_votes[
  score_votes$questionId %in% question_metadata$questionId,
]
mean_question_score_vote <- round(mean(question_score_votes$scoreVote), 2)
std_question_score_vote <- round(sd(question_score_votes$scoreVote), 2)
cat("Mean of scoreVote for the question",
    "'On a scale from 1 to 10, how would you rate the work-related stress?': ",
    mean_question_score_vote, "\n")
cat("Standard deviation of scoreVote for the question ",
    "'On a scale from 1 to 10, how would you rate the work-related stress?': ",
    std_question_score_vote, "\n")
```
### Question 2
Use the ggplot library (not R's built-in functions) to create a bar plot (horizontal or vertical orientation) of the number of companies per industry represented in this dataset (companyMetadata.csv), filtering for only the top 10 most frequent industries. (Hint: look through the dplyr library functions for help transforming your data set into the right set to use.) Make sure to customize the axes and title with meaningful labels, and that all text is clear and legible. 
```{r}
library(ggplot2)
library(dplyr)
industry_metadata <- company_metadata %>% group_by(industry)
top10_industries <- industry_metadata %>%
  summarise(count = n()) %>%
  top_n(10, count)
ggplot(top10_industries, aes(x = reorder(industry, -count), y = count)) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  theme(axis.text.x = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        axis.title = element_text(size = 16),
        plot.title = element_text(size = 16)) +
  xlab("Industry") +
  ylab("Number of Companies") +
  ggtitle("Number of Companies per Industry")

```
### Question 3
Create box plots showing distribution of scoreVote by the different score categories (i.e., scoreId). 
Hint: to execute this task, you'll need to join the scoreMeta table and scoreVotes table, using the `name` in the scoreMeta table as the x-axis. 
Again, make sure your axes, title, and labels are clear and meaningful. 
Give a sentence of analysis about this figure; at a high level, what does it mean/what can we learn from it? 
```{r}
joined_score_data <- score_votes %>% inner_join(
  score_metadata, by = c("scoreId" = "scoreId")
)
ggplot(joined_score_data, aes(x = name, y = scoreVote)) +
  geom_boxplot() +
  # Should also enlarge the test size
  xlab("Score Category") +
  ylab("Score Vote") +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1, size = 16),
    axis.text.y = element_text(size = 16),
    axis.title = element_text(size = 16),
    plot.title = element_text(size = 16)
  ) +
  ggtitle("Distribution of ScoreVote by Score Category")
```
From the image generated by using `ggplot`, we can learn that the distribution of all kinds of score category 
is generally similar, with the median value of scoreVote around 7. And all of these catgories have a large vairance since the boxes in the 
boxplot are quite long. This can be expalined as that people's opinions on these votes are somehow polarized.

### Question 4
Which company has the highest average "Wellbeing" score (scoreMetadata.csv, scoreVotes.csv, companyMetadata.csv)? 
```{r}
wellbeing_metadata <- score_metadata[score_metadata$name == "Wellbeing", ]
wellbeing_joined_data <- wellbeing_metadata %>%
  inner_join(score_votes, by = "scoreId")
company_joined_data <- wellbeing_joined_data %>%
  inner_join(company_metadata, by = "companyId")
average_wellbing_scores <- company_joined_data %>%
  group_by(companyId) %>%
  summarise(avg_score = mean(scoreVote))
highest_avg_score_company <- average_wellbing_scores %>%
  top_n(1, avg_score)
cat("The company with the highest average Wellbeing score is: ",
    highest_avg_score_company$companyId, "\n")
```

### Question 5
Compare Happiness Index (hiVote) between the two industries ARTS_ENTERTAINMENT_RECREATION and FINANCIAL_SERVICES_INSURANCE. 
Write a hypothesis, predicting which of the two industries will have a higher hiVote score. 
Conduct a t-test to evaluate your hypothesis: report the mean hiVote for the industries and the p-value,
identifying whether the results are significant based on a significance level of 0.05. 
Write a sentence describing the result of your statistical findings on your chosen hypothesis. 

Bascially, the happiness index of the arts, entertainment, and recreation industry is higher than that of the financial services and insurance industry.
Thus, we may come up witht the null hypothesis that there is no distinguishable difference between the happiness index of the two industries, 
and the alternative hypothesis that the happiness index of the arts, entertainment, and recreation industry is higher than that of the financial services and insurance industry.
```{r}
arts_entertainment_recreation <- company_metadata[
  company_metadata$industry == "ARTS_ENTERTAINMENT_RECREATION",
]
financial_services_insurance <- company_metadata[
  company_metadata$industry == "FINANCIAL_SERVICES_INSURANCE",
]

combined_data <- hi_votes %>%
  inner_join(company_metadata, by = "companyId") %>%
  filter(
    industry %in% c("ARTS_ENTERTAINMENT_RECREATION", "FINANCIAL_SERVICES_INSURANCE")
  )

mean_hi_votes_combined <- combined_data %>%
  group_by(industry) %>%
  summarise(mean_hi_vote = mean(hiVote))

t_test_result <- t.test(
  hiVote ~ industry,
  data = combined_data,
  var.equal = FALSE
)

cat("Mean hiVote for ARTS_ENTERTAINMENT_RECREATION: ",
    mean_hi_votes_combined$mean_hi_vote[1], "\n")
cat("Mean hiVote for FINANCIAL_SERVICES_INSURANCE: ",
    mean_hi_votes_combined$mean_hi_vote[2], "\n")
cat("P-value: ", t_test_result$p.value, "\n")
print(t_test_result)
```
The p-value is 0.01828607, which is less than the significance level of 0.05.
Therefore, we reject the null hypothesis and conclude that there is some kinds of relationship between the arts, entertainment, and recreation industry and the financial services industry. 
The happiness index of the arts, entertainment, and recreation industry is higher than that of the financial services and insurance industry.

### Question 6
Which company has the "happiest" employees? How would you justify that? (This question does not have one right answer.)

The most straightforward way is to compare the average happiness index of employees in different companies.
```{r}
happiest_company <- hi_votes %>%
  group_by(companyId) %>%
  summarise(mean_hi_vote = mean(hiVote)) %>%
  top_n(1, mean_hi_vote)
cat("The company with the happiest employees is: ",
    happiest_company$companyId, "\n")
```

### Question 7
For the company you chose in the previous question, show how analyzing the data differently would give you a different finding.

Another way to determine which company has the happiest employees is to calculate the median happiness index of employees in different companies.
```{r}
happiest_company_median <- hi_votes %>%
  group_by(companyId) %>%
  summarise(median_hi_vote = median(hiVote)) %>%
  top_n(1, median_hi_vote)
cat("The company with the happiest employees (based on median) is: ",
    happiest_company_median$companyId, "\n")
```

The company with the happiest employees based on the mean and median happiness index may not be the same since we have several companies having the same largest median happiness index.

### Question 8 HI and industry 
a. Run a one-way ANOVA to determine whether significant differences in mean hiVote exist across industries. Report the p-value and interpret the results.

b. Imagine (regardless of the outcome of the previous step) that your results indicate a statistically significant difference. 
In class, we've discussed strategies for post-hoc analysis, including interpreting visualizations and conducting post-hoc testing. 
Suppose you simply ran Tukey's test as your post-hoc analysis. How many comparisons would that statistical test be making? 

c. In a sentence or two, what might be a better analysis step?

**a.**
```{r}
company_voting_data <- hi_votes %>%
  inner_join(company_metadata, by = "companyId")
anova_result <- aov(hiVote ~ industry, data = company_voting_data)
summary(anova_result)
```
p-value: <<2e-16, which is far below the significance level (alpha=0.05).
Conclusion: Reject the null hypothesis. There are significant differences in mean hiVote scores across industries.
F-value: The F-statistic is 2456, which is very large, indicating that the variance between industries is much greater than the variance within industries.

**b.**
We know that the degree of freedom in the previous ANOVA summary is 15, which is equivalent to the number of industries minus 1. 
Thus, the number of distinct industries is 16.
The number of comparisons in Tukey's test is equal to the number of distinct industries multiplied by the number of distinct industries minus 1, divided by 2, which is
$$
\frac{16 \times 15}{2} = 120
$$

**c.**
A better analysis may also use several more hypothesis tests to compare the mean hiVote scores between each pair of industries, including Chi-square test, t-test, etc.
Also, we can create a visualization to show the distribution of hiVote scores across different industries to analyze them more intuitively.

### Question 9
Run a linear model regressing the scoreVote on company time zone. 
Which of these time zones seems to have the strongest effect? 
Which has the most significant effect? 
In a sentence, explain the relationship between effect size and statistical significance.
```{r}
company_vote_score_data <- score_votes %>%
  inner_join(company_metadata, by = "companyId")
linear_regression_result <- lm(
  scoreVote ~ timezone, data = company_vote_score_data
)
summary(linear_regression_result)
```
From the result of the linear regression, we have:
Strongest Effect: timezoneEurope/London (-1.11338, largest magnitude).
Most Statistically Significant Effect: timezoneEurope/London (p-value: 2e-16, smallest p-value with largest absolute t-value).
The relationship between effect size and statistical significance is that Large effect sizes often correspond to high significance, but small effect sizes can still be significant if the sample size is large enough. 

## Part II - Reflecting on Data Collection and Analysis

### Question 1
Even though you did not collect this data yourself, this data was collected by someone somewhere. 
What are three risks of using data you are not familiar with and how would you mitigate them?

**Answer:**
Risk 1: Ethics and Privacy Concerns
The data collected by others may contin sensitive information of people and have potential privacy leak risks. 
The data may also be crawled from the internet without authorization, which may also be offensive when the data is used for analysis without permission.
Mitigation: A possible solution to avoid privacy disasters is to anonymize the data before sharing it with others, or to obtain the consent of the data owner before using it.
Also, we may check for the lisence of the data to make sure that the data is legally collected and open to the public.

Risk 2: Data Quality
The data collected by others may be of poor quality, for example, containing missing values, or the methods used to collect the data does not follow the statistical principles. Sometimes it is also possible that
the environment where the data is collected is not controlled, which may lead to bias and reliability issues.
Mitigation: Before perforing data analysis, we may first preprocess the data, remove some potential outliers and errors while also normalize the data to make sure that the data is in a good quality.
Also, we may check the origin of the data, for example the paper the data is published, to make sure the data is reliable and we may process it following the instrcutrions in the paper.

Risk 3: Potential Interpretation issues
Since the data is collected by othes we are not familiar with, we may only process and analyze the data based on our understanding of the data, which sometimes may be insuffcient and biased.
Mitigation: Before analyzing the data, we may first get acknowledged with the background of the data, for example, learning basic biology knowledge about DNA before processing single cell RNA sequence data.

### Question 2
Look through the data itself. What are three strengths and three weaknesses of this dataset and how it was collected? 
You could think about a range of characteristics like the format of the data, the way it is organized, the size of the data, the balance between classes of data points, and more.

**Answer:**
Strentgh 1: The dataset has a clear structure.
Each csv file in this dataset has some connections with others, such as sharing a same questionId or companyId, which helps us to using inner join to combine different data grams together.

Strength 2: Suffcient sample size.
The dataset contains enough quantity of data for analysis. The number of samples in the `score_votes` is 495924 and the number of rows of the `hi_votes` is 2302358, which is large enough to analyze and avoid bias or overfitting.

Strength 3: Following the naming conventions.
The dataset follows a clear naming convention, which makes it easier to understand the meaning of each column and each row in the dataset. 
For example, the column names in the `hi_votes` dataset, marking the ids, date, and happyness score, are clear and easy to understand.

Weakness 1: Biased distribution of data.
The `hi_votes` dataset is baised on the category of company it cantains. 
```{r}
result <- hi_votes %>%
  inner_join(company_metadata, by = "companyId") %>%
  group_by(industry) %>%
  summarise(vote_count = n())
print(result)
```
The proportion of the arts, entertainment, and recreation company in this datagram is much less than any other industries while financial services taks more than 1/3 of the data.

Weakneess 2: Missing values.
Some important information in this dataset is missing. For example, some `industry` columns in the `company_metadata` table are empty, which would affect the analysis on the category of voting from different industries. 

Weakness 3: Redundancy of data.
Some columns of the dataset is redundant, which does not meet the Normal Form principles of the database.
The colunes in the `score_metadata` table, for example, the `name` column and the `question` column, are redundant since the `name` column can be derived from the `question` column.

### Question 3
Happyforce is a platform that aims to measure employee engagement through a "happiness index." 
Based on the data presented in the dataset and the quantitative measures you computed, do you think Happyforce reliably measures employee satisfaction? 
To answer this question, clearly articulate your thesis and give at least three supporting reasons. 

**Answer:**

The Happyforce platform is a reliable measure of employee satisfaction.
Reason 1:
The comparison between Arts, Entertainment, and Recreation and Financial Services industries showed a great differences (p-value = 0.018), which means it is statistically distinguishable.
This result aligns with general expectations about workplace satisfaction in these sectors.

Reason 2:
The ANOVA test on the hiVote scores across different industries showed a significant difference in mean hiVote scores (p-value = 2e-16), meaning that different industries have different levels of employee satisfaction, which also supports the reliability of the Happyforce platform.

Reason 3:
Significant relationships between the workplace location and the satisfaction scores was shown in the linear regression process. The timezoneEurope/London has the strongest effect on the scoreVote, which means that the location of the company has a significant impact on employee satisfaction.
This result is consistent with the general understanding that the location of the company can affect employee satisfaction, which indicates that the Happyforce platform is reliable in measuring employee satisfaction.

### Question 4
Imagine that you are:
(a.) a nefarious company leader who wants to claim that everyone loves their job. 
How might you manipulate the data to support your claim? Provide quantitative evidence in support of the claim. 
(b.) a disgruntled company rival. 
How might you manipulate the data to claim that everyone hates their job? Provide quantitative evidence in support of the claim. 

**Answer:**
**(a.)**
to claim that everyone loves their job, the most straightforward way is to drop most of the data with low happyness index and only keep the data with high happyness index.
```{r}
hi_votes_high <- hi_votes %>%
  filter(hiVote >= 4)
average_manipulated_hi_vote <- mean(hi_votes_high$hiVote)
cat("Average manipulated hiVote: ", average_manipulated_hi_vote, "\n")
```

**(b.)**
To claim that everyone hates their job, the most straightforward way is to drop most of the data with high happyness index and only keep the data with low happyness index.
```{r}
hi_votes_low <- hi_votes %>%
  filter(hiVote < 3)
average_manipulated_hi_vote <- mean(hi_votes_low$hiVote)
cat("Average manipulated hiVote: ", average_manipulated_hi_vote, "\n")
```

### Question 5
Look back at the questions in Part 1. List three more interesting questions like the ones in that section (i.e., based on quantitative measures you can compute). 
Choose one and answer it. 

**Answer:**
Question 1: What is the relationship between the average scoreVote and the number of votes for each question?
Question 2: On average, which industry has the happiest employees?
Question 3: Create a bar plot showing the average hi votes for each industry.

I choose Question 3: Create a bar plot showing the average hi votes for each industry.
```{r}
company_vote_data <- hi_votes %>%
  inner_join(company_metadata, by = "companyId")
average_hi_votes_by_industry <- company_vote_data %>%
  group_by(industry) %>%
  summarise(avg_hi_vote = mean(hiVote))
ggplot(
  average_hi_votes_by_industry,
  aes(x = reorder(industry, -avg_hi_vote), y = avg_hi_vote)
) +
  geom_bar(stat = "identity") +
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  theme(axis.text.x = element_text(size = 16),
        axis.text.y = element_text(size = 16),
        axis.title = element_text(size = 16),
        plot.title = element_text(size = 16)) +
  xlab("Industry") +
  ylab("Average hiVote") +
  ggtitle("Average hiVote for each Industry")
```

